{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beac61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82ec3c",
   "metadata": {},
   "source": [
    "你现在需要确认保存文件名。\n",
    "\n",
    "此界面提示你输入要保存的文件名，默认是 /Users/zhuyq0719/.zshrc。\n",
    "\n",
    "操作步骤：\n",
    "直接按回车（Enter）键\n",
    "\n",
    "就会把内容保存到 .zshrc 文件中。\n",
    "\n",
    "然后会回到 nano 编辑界面，再按 Ctrl + X 退出编辑器。\n",
    "\n",
    "总结\n",
    "回车：保存到 .zshrc\n",
    "Ctrl + X：退出 nano\n",
    "保存后，记得输入以下命令让配置立即生效：\n",
    "\n",
    "复制\n",
    "source ~/.zshrc\n",
    "``\n",
    "sudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3f423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入环境变量\n",
    "\n",
    "# export LANGSMITH_TRACING=true\n",
    "# export LANGSMITH_API_KEY=\"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\"\n",
    "# export LANGSMITH_PROJECT=\"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbab03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "model = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请用百炼API Key将本行替换为：api_key=\"sk-xxx\"\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope SDK的base_url\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "openai_client = wrap_openai(model)\n",
    "\n",
    "# This is the retriever we will use in RAG\n",
    "# This is mocked out, but it could be anything we want\n",
    "def retriever(query: str):\n",
    "    results = [\"Harrison worked at Kensho\"]\n",
    "    return results\n",
    "\n",
    "# This is the end-to-end RAG chain.\n",
    "# It does a retrieval step then calls OpenAI\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        \n",
    "       \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db5a2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-4df5b966-376f-9536-afe8-fbaf7c133517', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Harrison worked at Kensho.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1745416669, model='qwen-plus', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=36, total_tokens=43, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"where did harrison work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "\n",
    "openai_client = wrap_openai(model)\n",
    "\n",
    "def retriever(query: str):\n",
    "    results = [\"Harrison worked at Kensho\"]\n",
    "    return results\n",
    "\n",
    "@traceable\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797cbbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-d4514420-048a-9fc9-ac26-d5f2d9cdb8ee', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Harrison worked at Kensho.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1745416955, model='qwen-plus', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=36, total_tokens=43, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"where did harrison work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b692b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.run_trees import RunTree\n",
    "\n",
    "client = model\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith\n",
    "# Ensure that the LANGSMITH_TRACING environment variables is set for @traceable to work\n",
    "@traceable(\n",
    "  run_type=\"llm\",\n",
    "  name=\"OpenAI Call Decorator\",\n",
    "  project_name=\"My Project2\"\n",
    ")\n",
    "def call_openai(\n",
    "  messages: list[dict], model: str = \"qwen-plus\"\n",
    ") -> str:\n",
    "  return client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=messages,\n",
    "  ).choices[0].message.content\n",
    "\n",
    "# Call the decorated function\n",
    "call_openai(messages)\n",
    "\n",
    "\n",
    "# You can also specify the Project via the project_name parameter\n",
    "# This will override the project_name specified in the @traceable decorator\n",
    "call_openai(\n",
    "  messages,\n",
    "  langsmith_extra={\"project_name\": \"My Overridden Project\"},\n",
    ")\n",
    "\n",
    "# The wrapped OpenAI client accepts all the same langsmith_extra parameters\n",
    "# as @traceable decorated functions, and logs traces to LangSmith automatically.\n",
    "# Ensure that the LANGSMITH_TRACING environment variables is set for the wrapper to work.\n",
    "from langsmith import wrappers\n",
    "wrapped_client = wrappers.wrap_openai(client)\n",
    "wrapped_client.chat.completions.create(\n",
    "  model=\"qwen-plus\",\n",
    "  messages=messages,\n",
    "  langsmith_extra={\"project_name\": \"My Project\"},\n",
    ")\n",
    "\n",
    "\n",
    "# Alternatively, create a RunTree object\n",
    "# You can set the project name using the project_name parameter\n",
    "rt = RunTree(\n",
    "  run_type=\"llm\",\n",
    "  name=\"OpenAI Call RunTree\",\n",
    "  inputs={\"messages\": messages},\n",
    "  project_name=\"My Project1\"\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "  model=\"qwen-plus\",\n",
    "  messages=messages,\n",
    ")\n",
    "# End and submit the run\n",
    "rt.end(outputs=chat_completion)\n",
    "rt.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f25d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some creative and catchy name ideas for a store that sells colorful socks:\\n\\n1. **Sole Mates** - A playful twist on \"soul mates,\" emphasizing the connection between feet and socks.\\n2. **Rainbow Toes** - Celebrates the vibrancy and colorfulness of your sock collection.\\n3. **Sock Pop** - Suggests fun, trendy, and vibrant styles.\\n4. **Feet Fest** - Implies a celebration for your feet with unique and colorful options.\\n5. **Chromatix Socks** - Combines \"chroma\" (color) with \"socks\" for an artistic flair.\\n6. **Happy Heels** - Focuses on bringing joy to everyday footwear.\\n7. **Pedal Palette** - Evokes the idea of painting your feet with colorful socks.\\n8. **Socktastic** - A fun, energetic name highlighting the excitement of socks.\\n9. **Color My Feet** - Simple and descriptive, emphasizing the colorful nature of your products.\\n10. **Step in Style** - Encourages customers to elevate their outfits one step at a time.\\n\\nChoose a name that reflects the personality of your brand while resonating with your target audience!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import Client\n",
    "\n",
    "# openai = Client()\n",
    "\n",
    "@traceable\n",
    "def format_prompt(subject):\n",
    "  return [\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\",\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"What's a good name for a store that sells {subject}?\"\n",
    "      }\n",
    "  ]\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def invoke_llm(messages):\n",
    "  return model.chat.completions.create(\n",
    "      messages=messages, model=\"qwen-plus\", temperature=0\n",
    "  )\n",
    "\n",
    "@traceable\n",
    "def parse_output(response):\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def run_pipeline():\n",
    "  messages = format_prompt(\"colorful socks\")\n",
    "  response = invoke_llm(messages)\n",
    "  return parse_output(response)\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d95355c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langsmith as ls\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "client = wrap_openai(model)\n",
    "\n",
    "@ls.traceable(run_type=\"tool\", name=\"Retrieve Context\")\n",
    "def my_tool(question: str) -> str:\n",
    "    return \"During this morning's meeting, we solved all world conflict.\"\n",
    "\n",
    "def chat_pipeline(question: str):\n",
    "    context = my_tool(question)\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "    ]\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus\", messages=messages\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "app_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}\n",
    "\n",
    "with ls.trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:\n",
    "    output = chat_pipeline(\"Can you summarize this morning's meetings?\")\n",
    "    rt.end(outputs={\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b18010a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This morning's meeting focused on addressing and resolving all world conflicts, achieving a significant and comprehensive solution.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "client = wrap_openai(model)\n",
    "\n",
    "@traceable(run_type=\"tool\", name=\"Retrieve Context\")\n",
    "def my_tool(question: str) -> str:\n",
    "  return \"During this morning's meeting, we solved all world conflict.\"\n",
    "\n",
    "@traceable(name=\"Chat Pipeline\")\n",
    "def chat_pipeline(question: str):\n",
    "  context = my_tool(question)\n",
    "  messages = [\n",
    "      { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "      { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "  ]\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      model=\"qwen-plus\", messages=messages\n",
    "  )\n",
    "  return chat_completion.choices[0].message.content\n",
    "\n",
    "chat_pipeline(\"Can you summarize this morning's meetings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fc69786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.run_trees import RunTree\n",
    "# This can be a user input to your app\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "# Create a top-level run\n",
    "pipeline = RunTree(\n",
    "  name=\"Chat Pipeline\",\n",
    "  run_type=\"chain\",\n",
    "  inputs={\"question\": question}\n",
    ")\n",
    "pipeline.post()\n",
    "# This can be retrieved in a retrieval step\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "messages = [\n",
    "  { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "  { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "]\n",
    "# Create a child run\n",
    "child_llm_run = pipeline.create_child(\n",
    "  name=\"OpenAI Call\",\n",
    "  run_type=\"llm\",\n",
    "  inputs={\"messages\": messages},\n",
    ")\n",
    "child_llm_run.post()\n",
    "# Generate a completion\n",
    "# client = openai.Client()\n",
    "chat_completion = model.chat.completions.create(\n",
    "  model=\"qwen-plus\", messages=messages\n",
    ")\n",
    "# End the runs and log them\n",
    "child_llm_run.end(outputs=chat_completion)\n",
    "child_llm_run.patch()\n",
    "pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\n",
    "pipeline.patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Type, TypeVar\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def traceable_cls(cls: Type[T]) -> Type[T]:\n",
    "    \"\"\"Instrument all public methods in a class.\"\"\"\n",
    "\n",
    "    def wrap_method(name: str, method: Any) -> Any:\n",
    "        if callable(method) and not name.startswith(\"__\"):\n",
    "            return traceable(name=f\"{cls.__name__}.{name}\")(method)\n",
    "        return method\n",
    "\n",
    "    # Handle __dict__ case\n",
    "    for name in dir(cls):\n",
    "        if not name.startswith(\"_\"):\n",
    "            try:\n",
    "                method = getattr(cls, name)\n",
    "                setattr(cls, name, wrap_method(name, method))\n",
    "            except AttributeError:\n",
    "                # Skip attributes that can't be set (e.g., some descriptors)\n",
    "                pass\n",
    "\n",
    "    # Handle __slots__ case\n",
    "    if hasattr(cls, \"__slots__\"):\n",
    "        for slot in cls.__slots__:  # type: ignore[attr-defined]\n",
    "            if not slot.startswith(\"__\"):\n",
    "                try:\n",
    "                    method = getattr(cls, slot)\n",
    "                    setattr(cls, slot, wrap_method(slot, method))\n",
    "                except AttributeError:\n",
    "                    # Skip slots that don't have a value yet\n",
    "                    pass\n",
    "\n",
    "    return cls\n",
    "\n",
    "\n",
    "\n",
    "@traceable_cls\n",
    "class MyClass:\n",
    "    def __init__(self, some_val: int):\n",
    "        self.some_val = some_val\n",
    "\n",
    "    def combine(self, other_val: int):\n",
    "        return self.some_val + other_val\n",
    "\n",
    "# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r\n",
    "MyClass(13).combine(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b85b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/ys10xl6112v_3491n6xnt8sm0000gn/T/ipykernel_55751/1032392239.py:61: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  patch_run(child_run_id, chat_completion.dict())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from uuid import uuid4\n",
    "\n",
    "# Send your API Key in the request headers\n",
    "headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n",
    "\n",
    "def post_run(run_id, name, run_type, inputs, parent_id=None):\n",
    "    \"\"\"Function to post a new run to the API.\"\"\"\n",
    "    data = {\n",
    "        \"id\": run_id.hex,\n",
    "        \"name\": name,\n",
    "        \"run_type\": run_type,\n",
    "        \"inputs\": inputs,\n",
    "        \"start_time\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "    if parent_id:\n",
    "        data[\"parent_run_id\"] = parent_id.hex\n",
    "    requests.post(\n",
    "        \"https://api.smith.langchain.com/runs\", # Update appropriately for self-hosted installations or the EU region\n",
    "        json=data,\n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "def patch_run(run_id, outputs):\n",
    "    \"\"\"Function to patch a run with outputs.\"\"\"\n",
    "    requests.patch(\n",
    "        f\"https://api.smith.langchain.com/runs/{run_id}\",\n",
    "        json={\n",
    "            \"outputs\": outputs,\n",
    "            \"end_time\": datetime.now(timezone.utc).isoformat(),\n",
    "        },\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "\n",
    "# This can be a user input to your app\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "\n",
    "# This can be retrieved in a retrieval step\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\n",
    "]\n",
    "\n",
    "# Create parent run\n",
    "parent_run_id = uuid4()\n",
    "post_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})\n",
    "\n",
    "# Create child run\n",
    "child_run_id = uuid4()\n",
    "post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n",
    "\n",
    "# Generate a completion\n",
    "\n",
    "chat_completion = model.chat.completions.create(model=\"qwen-plus\", messages=messages)\n",
    "\n",
    "# End runs\n",
    "patch_run(child_run_id, chat_completion.dict())\n",
    "patch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef7142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7208131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "from requests_toolbelt import MultipartEncoder\n",
    "\n",
    "\n",
    "def create_dotted_order(\n",
    "    start_time: Optional[datetime] = None, run_id: Optional[uuid.UUID] = None\n",
    ") -> str:\n",
    "    \"\"\"Create a dotted order string for run ordering and hierarchy.\n",
    "\n",
    "    The dotted order is used to establish the sequence and relationships between runs.\n",
    "    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.\n",
    "    \"\"\"\n",
    "    st = start_time or datetime.now(timezone.utc)\n",
    "    id_ = run_id or uuid.uuid4()\n",
    "    return f\"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}\"\n",
    "\n",
    "\n",
    "def create_run_base(\n",
    "    name: str, run_type: str, inputs: dict, start_time: datetime\n",
    ") -> dict:\n",
    "    \"\"\"Create the base structure for a run.\"\"\"\n",
    "    run_id = uuid.uuid4()\n",
    "    return {\n",
    "        \"id\": str(run_id),\n",
    "        \"trace_id\": str(run_id),\n",
    "        \"name\": name,\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"inputs\": inputs,\n",
    "        \"run_type\": run_type,\n",
    "    }\n",
    "\n",
    "\n",
    "def construct_run(\n",
    "    name: str,\n",
    "    run_type: str,\n",
    "    inputs: dict,\n",
    "    parent_dotted_order: Optional[str] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Construct a run dictionary with the given parameters.\n",
    "\n",
    "    This function creates a run with a unique ID and dotted order, establishing its place\n",
    "    in the trace hierarchy if it's a child run.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now(timezone.utc)\n",
    "    run = create_run_base(name, run_type, inputs, start_time)\n",
    "\n",
    "    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run[\"id\"]))\n",
    "    if parent_dotted_order:\n",
    "        current_dotted_order = f\"{parent_dotted_order}.{current_dotted_order}\"\n",
    "        run[\"trace_id\"] = parent_dotted_order.split(\".\")[0].split(\"Z\")[1]\n",
    "        run[\"parent_run_id\"] = parent_dotted_order.split(\".\")[-1].split(\"Z\")[1]\n",
    "    run[\"dotted_order\"] = current_dotted_order\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def serialize_run(operation: str, run_data: dict) -> List[tuple]:\n",
    "    \"\"\"Serialize a run for the multipart request.\n",
    "\n",
    "    This function separates the run data into parts for efficient transmission and storage.\n",
    "    The main run data and optional fields (inputs, outputs, events) are serialized separately.\n",
    "    \"\"\"\n",
    "    run_id = run_data.get(\"id\", str(uuid.uuid4()))\n",
    "\n",
    "    # Separate optional fields\n",
    "    inputs = run_data.pop(\"inputs\", None)\n",
    "    outputs = run_data.pop(\"outputs\", None)\n",
    "    events = run_data.pop(\"events\", None)\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Serialize main run data\n",
    "    run_data_json = json.dumps(run_data).encode(\"utf-8\")\n",
    "    parts.append(\n",
    "        (\n",
    "            f\"{operation}.{run_id}\",\n",
    "            (\n",
    "                None,\n",
    "                run_data_json,\n",
    "                \"application/json\",\n",
    "                {\"Content-Length\": str(len(run_data_json))},\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Serialize optional fields\n",
    "    for key, value in [(\"inputs\", inputs), (\"outputs\", outputs), (\"events\", events)]:\n",
    "        if value:\n",
    "            serialized_value = json.dumps(value).encode(\"utf-8\")\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"{operation}.{run_id}.{key}\",\n",
    "                    (\n",
    "                        None,\n",
    "                        serialized_value,\n",
    "                        \"application/json\",\n",
    "                        {\"Content-Length\": str(len(serialized_value))},\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "def batch_ingest_runs(\n",
    "    api_url: str,\n",
    "    api_key: str,\n",
    "    posts: Optional[List[dict]] = None,\n",
    "    patches: Optional[List[dict]] = None,\n",
    ") -> None:\n",
    "    \"\"\"Ingest multiple runs in a single batch request.\n",
    "\n",
    "    This function handles both creating new runs (posts) and updating existing runs (patches).\n",
    "    It's more efficient for ingesting multiple runs compared to individual API calls.\n",
    "    \"\"\"\n",
    "    boundary = uuid.uuid4().hex\n",
    "    all_parts = []\n",
    "\n",
    "    for operation, runs in zip((\"post\", \"patch\"), (posts, patches)):\n",
    "        if runs:\n",
    "            all_parts.extend(\n",
    "                [part for run in runs for part in serialize_run(operation, run)]\n",
    "            )\n",
    "\n",
    "    encoder = MultipartEncoder(fields=all_parts, boundary=boundary)\n",
    "    headers = {\"Content-Type\": encoder.content_type, \"x-api-key\": api_key}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{api_url}/runs/multipart\", data=encoder, headers=headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        print(\"Successfully ingested runs.\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error ingesting runs: {e}\")\n",
    "        # In a production environment, you might want to log this error or handle it more robustly\n",
    "\n",
    "\n",
    "# Configure API URL and key\n",
    "# For production use, consider using a configuration file or environment variables\n",
    "api_url = \"https://api.smith.langchain.com\"\n",
    "api_key = os.environ.get(\"LANGSMITH_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"LANGSMITH_API_KEY environment variable is not set\")\n",
    "\n",
    "# Create a parent run\n",
    "parent_run = construct_run(\n",
    "    name=\"Parent Run\",\n",
    "    run_type=\"chain\",\n",
    "    inputs={\"main_question\": \"Tell me about France\"},\n",
    ")\n",
    "\n",
    "# Create a child run, linked to the parent\n",
    "child_run = construct_run(\n",
    "    name=\"Child Run\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"question\": \"What is the capital of France?\"},\n",
    "    parent_dotted_order=parent_run[\"dotted_order\"],\n",
    ")\n",
    "\n",
    "# First, post the runs to create them\n",
    "posts = [parent_run, child_run]\n",
    "batch_ingest_runs(api_url, api_key, posts=posts)\n",
    "\n",
    "# Then, update the runs with their end times and any outputs\n",
    "child_run_update = {\n",
    "    **child_run,\n",
    "    \"end_time\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"outputs\": {\"answer\": \"Paris is the capital of France.\"},\n",
    "}\n",
    "parent_run_update = {\n",
    "    **parent_run,\n",
    "    \"end_time\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"outputs\": {\"summary\": \"Discussion about France, including its capital.\"},\n",
    "}\n",
    "patches = [parent_run_update, child_run_update]\n",
    "batch_ingest_runs(api_url, api_key, patches=patches)\n",
    "\n",
    "# Note: This example requires the `requests` and `requests_toolbelt` libraries.\n",
    "# You can install them using pip:\n",
    "# pip install requests requests_toolbelt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
